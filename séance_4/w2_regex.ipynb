{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regex (Regular Expressions) for Web Scraping\n",
        "\n",
        "What is **regex** in Python with **practical scraping-style examples** (extracting emails, prices, dates, IDs, etc.).\n",
        "\n",
        "## Learning goals\n",
        "- Understand what regex is and when to use it\n",
        "- Read common patterns (character classes, quantifiers, groups)\n",
        "- Use `re.search`, `re.findall`, `re.sub`, `re.split`\n",
        "- Apply regex to messy text from the web (snippets, HTML, logs)\n",
        "\n",
        "> Tip: Regex is powerful but should not replace HTML parsing. For web scraping, **use BeautifulSoup to get the right text/attributes**, then use regex to **clean** or **extract** patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) What is regex?\n",
        "\n",
        "A **regular expression** is a pattern used to **match text**.\n",
        "\n",
        "Common scraping tasks:\n",
        "- Find **emails**, **phone numbers**, **prices**, **postal codes**\n",
        "- Extract **IDs** from URLs\n",
        "- Normalize text (remove extra spaces, convert formats)\n",
        "- Validate or filter content\n",
        "\n",
        "Python regex lives in the built-in module: `re`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) The 4 most used functions\n",
        "\n",
        "- `re.search(pattern, text)` → finds the **first** match anywhere\n",
        "- `re.findall(pattern, text)` → returns **all** matches\n",
        "- `re.sub(pattern, repl, text)` → **replaces** matches\n",
        "- `re.split(pattern, text)` → splits text by a pattern\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"My email is alice@example.com and my backup is bob.smith@company.co.uk\"\n",
        "pattern = r\"[\\w.-]+@[\\w.-]+\\.\\w+\"  # a simple (not perfect) email regex\n",
        "\n",
        "m = re.search(pattern, text)\n",
        "print(\"search ->\", m.group(0))\n",
        "\n",
        "all_emails = re.findall(pattern, text)\n",
        "print(\"findall ->\", all_emails)\n",
        "\n",
        "masked = re.sub(pattern, \"<EMAIL>\", text)\n",
        "print(\"sub ->\", masked)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Core building blocks (cheat sheet)\n",
        "\n",
        "### Character classes\n",
        "- `.` any character (except newline)\n",
        "- `\\d` digit, `\\D` non-digit\n",
        "- `\\w` word char (letters/digits/_), `\\W` non-word\n",
        "- `\\s` whitespace, `\\S` non-whitespace\n",
        "- `[abc]` one of a/b/c\n",
        "- `[a-z]` range\n",
        "- `[^a-z]` NOT in the range\n",
        "\n",
        "### Quantifiers\n",
        "- `?` 0 or 1\n",
        "- `*` 0 or more\n",
        "- `+` 1 or more\n",
        "- `{n}` exactly n\n",
        "- `{n,}` at least n\n",
        "- `{n,m}` between n and m\n",
        "\n",
        "### Anchors\n",
        "- `^` start of string\n",
        "- `$` end of string\n",
        "- `\\b` word boundary\n",
        "\n",
        "### Groups\n",
        "- `( ... )` capturing group\n",
        "- `(?: ... )` non-capturing group\n",
        "- `(?P<name> ... )` named capturing group\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples = [\n",
        "    \"Order #A-10293 shipped\",\n",
        "    \"Order #B-7 shipped\",\n",
        "    \"No order here\",\n",
        "]\n",
        "\n",
        "pattern = r\"#([A-Z])-([0-9]+)\"  # capture letter and digits\n",
        "\n",
        "for s in samples:\n",
        "    m = re.search(pattern, s)\n",
        "    if m:\n",
        "        print(s, \"->\", m.group(0), \"| groups:\", m.group(1), m.group(2))\n",
        "    else:\n",
        "        print(s, \"-> no match\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Greedy vs lazy\n",
        "\n",
        "Quantifiers like `*` and `+` are **greedy** by default (they match as much as possible).\n",
        "Use `*?` or `+?` to make them **lazy** (match as little as possible).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "html = \"<div>Price: <b>19.99</b> EUR</div><div>Price: <b>5.00</b> EUR</div>\"\n",
        "\n",
        "greedy = re.findall(r\"<b>(.*)</b>\", html)\n",
        "lazy = re.findall(r\"<b>(.*?)</b>\", html)\n",
        "\n",
        "print(\"greedy:\", greedy)\n",
        "print(\"lazy:\", lazy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Flags (multiline, ignore case, dotall)\n",
        "\n",
        "- `re.IGNORECASE` (or `re.I`): case-insensitive\n",
        "- `re.MULTILINE` (or `re.M`): `^` and `$` work per line\n",
        "- `re.DOTALL` (or `re.S`): `.` matches newlines too\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"Name: Alice\\nname: Bob\\nNAME: Charlie\"\"\"\n",
        "print(re.findall(r\"^name:\\s*(\\w+)\", text, flags=re.I | re.M))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Practical examples\n",
        "\n",
        "### A) Extract prices (€, $, with comma or dot)\n",
        "\n",
        "Real-world issue: prices may appear as `9,99€`, `€9.99`, `$ 1,200.50`, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Promo: 9,99€ now! Old price: €12.50. US format: $ 1,200.50\"\n",
        "\n",
        "# This pattern captures currency + number with optional thousand separators\n",
        "price_pattern = r\"(?P<currency>€|\\$)\\s?(?P<amount>\\d{1,3}(?:[\\.,]\\d{3})*(?:[\\.,]\\d{2})?)|(?P<amount2>\\d+(?:[\\.,]\\d{2})?)\\s?(?P<currency2>€|\\$)\"\n",
        "\n",
        "matches = list(re.finditer(price_pattern, text))\n",
        "for m in matches:\n",
        "    d = m.groupdict()\n",
        "    currency = d.get('currency') or d.get('currency2')\n",
        "    amount = d.get('amount') or d.get('amount2')\n",
        "    print(\"price ->\", currency, amount, \"| raw:\", m.group(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B) Extract dates in multiple formats\n",
        "\n",
        "Examples: `2026-01-20`, `20/01/2026`, `Jan 20, 2026`.\n",
        "\n",
        "> In scraping, you often need regex **just to detect/extract**, then use Pandas to parse into datetimes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Release: 2026-01-20 | Updated: 20/01/2026 | Blog: Jan 20, 2026\"\n",
        "\n",
        "date_pattern = r\"\\b(\\d{4}-\\d{2}-\\d{2}|\\d{2}/\\d{2}/\\d{4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4})\\b\"\n",
        "print(re.findall(date_pattern, text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C) Extract IDs from URLs\n",
        "\n",
        "Useful for e-commerce product IDs, user IDs, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://site.com/product/12345?ref=home\",\n",
        "    \"https://site.com/product/98765\",\n",
        "    \"https://site.com/about\",\n",
        "]\n",
        "\n",
        "pat = r\"/product/(\\d+)\"\n",
        "for u in urls:\n",
        "    m = re.search(pat, u)\n",
        "    print(u, \"->\", m.group(1) if m else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### D) Clean messy whitespace (common after scraping)\n",
        "\n",
        "Scraped text often contains newlines, tabs, multiple spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = \"\\n\\t  This   is\\n a    messy\\t\\ttext.   \"\n",
        "clean = re.sub(r\"\\s+\", \" \", raw).strip()\n",
        "print(\"raw:\", repr(raw))\n",
        "print(\"clean:\", repr(clean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### E) Extract phone numbers (simple FR-style demo)\n",
        "\n",
        "**Note:** phone regex can get complex internationally; this is a *teaching example*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Call us: 06 12 34 56 78 or +33 6 12 34 56 78\"\n",
        "pat = r\"(?:\\+33\\s?)?0?6(?:\\s?\\d{2}){4}\"\n",
        "print(re.findall(pat, text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Bonus: Lookarounds (when you want context but don't want to capture it)\n",
        "\n",
        "- `(?=...)` positive lookahead\n",
        "- `(?!...)` negative lookahead\n",
        "- `(?<=...)` positive lookbehind\n",
        "- `(?<!...)` negative lookbehind\n",
        "\n",
        "Example: extract numbers **only** if followed by `€`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"Items: 12€ and 30 dollars and 5€\"\n",
        "print(re.findall(r\"\\d+(?=€)\", text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Mini scraping-style workflow (static HTML string)\n",
        "\n",
        "In real scraping:\n",
        "1. Use `requests` to download HTML\n",
        "2. Use **BeautifulSoup** to select the right elements\n",
        "3. Use regex to extract/clean inside that text\n",
        "\n",
        "Here we simulate step (2) by using a plain string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "page_text = \"\"\"\n",
        "Product: Super Mug\\n\n",
        "Price: 9,99€\\n\n",
        "Contact: shop@my-store.com\\n\n",
        "Product ID: SKU-AB1234\\n\n",
        "\"\"\"\n",
        "\n",
        "email = re.search(r\"[\\w.-]+@[\\w.-]+\\.\\w+\", page_text).group(0)\n",
        "price = re.search(r\"\\d+(?:[\\.,]\\d{2})?\\s?€\", page_text).group(0)\n",
        "sku = re.search(r\"SKU-[A-Z]{2}\\d{4}\", page_text).group(0)\n",
        "\n",
        "print(\"email:\", email)\n",
        "print(\"price:\", price)\n",
        "print(\"sku:\", sku)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Exercises\n",
        "\n",
        "1. Extract all hashtags from a text (e.g. `#PANDAS`, `#WEB_SCRAPING`).\n",
        "2. Extract all numbers that look like percentages (e.g. `20%`, `50 %`).\n",
        "3. Replace multiple spaces/newlines with a single space.\n",
        "4. Extract the domain names from a list of URLs.\n",
        "\n",
        "### Starter data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"This course covers #WEB_SCRAPING, #PANDAS, and #REGEX. Grading: 20% MCQ, 30 % CC, 50% EXAM.\"\n",
        "urls = [\n",
        "    \"https://www.wikipedia.org/wiki/Pandas\",\n",
        "    \"http://example.com/path/to/page\",\n",
        "    \"https://sub.domain.co.uk/index.html\",\n",
        "]\n",
        "messy = \"Hello\\n\\n   world\\t\\tthis   is   messy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solutions (uncomment to reveal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Hashtags\n",
        "# print(re.findall(r\"#[A-Z_]+\", text))\n",
        "\n",
        "# 2) Percentages\n",
        "# print(re.findall(r\"\\b\\d+\\s?%\\b\", text))\n",
        "\n",
        "# 3) Normalize whitespace\n",
        "# print(re.sub(r\"\\s+\", \" \", messy).strip())\n",
        "\n",
        "# 4) Domains from URLs (simple)\n",
        "# for u in urls:\n",
        "#     m = re.search(r\"https?://([^/]+)\", u)\n",
        "#     print(u, \"->\", m.group(1) if m else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Summary\n",
        "\n",
        "- Use BeautifulSoup to select the right chunk of HTML/text\n",
        "- Use regex to extract patterns (emails, prices, IDs, dates)\n",
        "- Use `re.sub` to clean messy text\n",
        "- Watch out for **greedy vs lazy** matching"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
